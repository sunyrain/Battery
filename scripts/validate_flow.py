#!/usr/bin/env python
"""
Flow Matching æ¨¡å‹éªŒè¯è„šæœ¬

éªŒè¯å†…å®¹:
1. è½¨è¿¹é‡å»ºè´¨é‡ - ç»™å®š (z_0, z_1)ï¼Œé¢„æµ‹çš„è½¨è¿¹ z_T æ˜¯å¦æ¥è¿‘ z_1
2. å¥åº·è¯„åˆ†æ›²çº¿ - é¢„æµ‹çš„é€€åŒ–æ›²çº¿æ˜¯å¦ç¬¦åˆç‰©ç†æ„ä¹‰
3. æ½œç©ºé—´å¯è§†åŒ– - é™ç»´åæŸ¥çœ‹è½¨è¿¹åˆ†å¸ƒ
4. RUL é¢„æµ‹ç²¾åº¦ - å¯¹å·²çŸ¥ç”Ÿå‘½å‘¨æœŸçš„æ ·æœ¬è¯„ä¼° RUL é¢„æµ‹

ä½¿ç”¨æ–¹æ³•:
    python scripts/validate_flow.py --checkpoint experiments/flow_matching/checkpoints/best_model.pt
"""

import os
import sys
import argparse
import logging
import torch
import numpy as np
import pandas as pd
from pathlib import Path

sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.flow_matching.models.flow_model import BatteryFlowModel, BatteryFlowConfig
from src.flow_matching.core.ode_solver import create_solver
from src.flow_matching.data.latent_cache import LatentCache
from src.flow_matching.utils.config import load_config
from src.smartwavev9 import DeltaBatteryModel

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


def load_encoder(checkpoint_path: str, device: torch.device):
    """åŠ è½½é¢„è®­ç»ƒ Encoder"""
    checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)
    
    # è·å–é…ç½®
    if 'config' in checkpoint and 'model' in checkpoint['config']:
        model_cfg = checkpoint['config']['model']['model_config']
    else:
        model_cfg = {'d_model': 256, 'nhead': 4, 'num_layers': 1}
    
    encoder = DeltaBatteryModel(
        input_dim=model_cfg.get('input_dim', 1),
        d_model=model_cfg.get('d_model', 256),
        nhead=model_cfg.get('nhead', 4),
        num_layers=model_cfg.get('num_layers', 1),
        ROPE_max_len=model_cfg.get('ROPE_max_len', 5000),
        num_classes=model_cfg.get('num_classes', 4),
        task_type=model_cfg.get('task_type', 'classification'),
        max_level=model_cfg.get('max_level', 6),
        wavelet=model_cfg.get('wavelet', 'sym4'),
        patch_size=model_cfg.get('patch_size', 10),
        stride=model_cfg.get('stride', 10),
        dropout=model_cfg.get('dropout', 0.2),
    )
    
    state_dict = checkpoint.get('model_state_dict', checkpoint)
    if any(k.startswith('model.') for k in state_dict.keys()):
        state_dict = {k[6:]: v for k, v in state_dict.items() if k.startswith('model.')}
    
    # é¢„åˆ›å»ºæŠ•å½±å±‚
    for key in state_dict.keys():
        if key.startswith('freq_branch.channel_projections.') and key.endswith('.weight'):
            name = key.split('.')[2]
            weight = state_dict[key]
            encoder.freq_branch.channel_projections[name] = torch.nn.Linear(weight.shape[1], weight.shape[0])
    
    # è¿‡æ»¤æ—§å‚æ•°
    keys_to_ignore = {'beta1', 'beta2_offset'}
    state_dict = {k: v for k, v in state_dict.items() if k not in keys_to_ignore}
    
    encoder.load_state_dict(state_dict, strict=False)
    encoder.to(device)
    encoder.eval()
    
    return encoder, model_cfg.get('d_model', 256)


def load_flow_model(checkpoint_path: str, encoder: torch.nn.Module, latent_dim: int, device: torch.device):
    """åŠ è½½ Flow Matching æ¨¡å‹"""
    checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)
    
    # ä» state_dict æ¨æ–­æ¨¡å‹é…ç½®
    state_dict = checkpoint.get('ema_state_dict', checkpoint.get('model_state_dict', {}))
    
    # æ¨æ–­ cond_embed_dim ä» condition_embedding.fusion.0.bias å½¢çŠ¶
    cond_embed_dim = 64  # é»˜è®¤å€¼
    if 'condition_embedding.fusion.0.bias' in state_dict:
        cond_embed_dim = state_dict['condition_embedding.fusion.0.bias'].shape[0]
    
    # æ¨æ–­ time_embed_dim ä» velocity_net.time_embed.mlp.2.bias å½¢çŠ¶
    time_embed_dim = 64  # é»˜è®¤å€¼
    if 'velocity_net.time_embed.mlp.2.bias' in state_dict:
        time_embed_dim = state_dict['velocity_net.time_embed.mlp.2.bias'].shape[0]
    
    # æ¨æ–­ hidden_dim ä» velocity_net.net.0.weight å½¢çŠ¶
    hidden_dim = 256  # é»˜è®¤å€¼
    if 'velocity_net.net.0.weight' in state_dict:
        hidden_dim = state_dict['velocity_net.net.0.weight'].shape[0]
    
    logger.info(f"ä»æƒé‡æ¨æ–­é…ç½®: latent_dim={latent_dim}, hidden_dim={hidden_dim}, cond_embed_dim={cond_embed_dim}, time_embed_dim={time_embed_dim}")
    
    config = BatteryFlowConfig(
        latent_dim=latent_dim,
        hidden_dim=hidden_dim,
        cond_embed_dim=cond_embed_dim,
        time_embed_dim=time_embed_dim,
        num_layers=4,
        max_cycle=200,
        lightweight=True,
    )
    
    model = BatteryFlowModel(config, encoder)
    
    # åŠ è½½æ¨¡å‹æƒé‡ - å¤„ç†ä¸åŒçš„ä¿å­˜æ ¼å¼
    if 'ema_state_dict' in checkpoint:
        state_dict = checkpoint['ema_state_dict']
        # æ£€æŸ¥æ˜¯å¦åŒ…å« velocity_net å‰ç¼€
        if any(k.startswith('velocity_net.') for k in state_dict.keys()):
            # æ•´ä¸ªæ¨¡å‹çš„ state_dictï¼Œç›´æ¥åŠ è½½åˆ° model
            model.load_state_dict(state_dict, strict=False)
        else:
            # åªæ˜¯ velocity_net çš„ state_dict
            model.velocity_net.load_state_dict(state_dict)
        logger.info("ä½¿ç”¨ EMA æƒé‡")
    elif 'model_state_dict' in checkpoint:
        state_dict = checkpoint['model_state_dict']
        # æ£€æŸ¥æ˜¯å¦åŒ…å« velocity_net å‰ç¼€
        if any(k.startswith('velocity_net.') for k in state_dict.keys()):
            # æ•´ä¸ªæ¨¡å‹çš„ state_dictï¼Œç›´æ¥åŠ è½½åˆ° model
            model.load_state_dict(state_dict, strict=False)
        else:
            # åªæ˜¯ velocity_net çš„ state_dict
            model.velocity_net.load_state_dict(state_dict)
        logger.info("ä½¿ç”¨æ¨¡å‹æƒé‡")
    
    model.to(device)
    model.eval()
    
    return model, config


@torch.no_grad()
def validate_trajectory_reconstruction(model, latent_vectors, cycles, num_samples=100, device='cuda'):
    """
    éªŒè¯ 1: è½¨è¿¹é‡å»ºè´¨é‡
    
    å¯¹äºéšæœºé‡‡æ ·çš„ (z_0, z_1) å¯¹ï¼Œä» z_0 ç§¯åˆ†åˆ° t=1ï¼Œæ£€æŸ¥æ˜¯å¦æ¥è¿‘ z_1
    """
    logger.info("=" * 60)
    logger.info("éªŒè¯ 1: è½¨è¿¹é‡å»ºè´¨é‡")
    logger.info("=" * 60)
    
    n = len(cycles)
    errors = []
    relative_errors = []
    
    for _ in range(num_samples):
        # éšæœºé€‰æ‹©ä¸€å¯¹
        idx_0 = np.random.randint(n)
        cycle_0 = cycles[idx_0].item()
        
        # æ‰¾ä¸€ä¸ªè¾ƒæ™šçš„ cycle
        valid_mask = (cycles - cycle_0 >= 10) & (cycles - cycle_0 <= 50)
        valid_indices = torch.where(valid_mask)[0]
        
        if len(valid_indices) == 0:
            continue
        
        idx_1 = valid_indices[np.random.randint(len(valid_indices))].item()
        cycle_1 = cycles[idx_1].item()
        
        z_0 = latent_vectors[idx_0:idx_0+1].to(device)
        z_1 = latent_vectors[idx_1:idx_1+1].to(device)
        
        # ç§¯åˆ†é¢„æµ‹
        t_span = torch.linspace(0, 1, 50, device=device)
        trajectory = model.predict_trajectory(z_0, t_span)
        z_pred = trajectory[-1]  # æœ€ç»ˆé¢„æµ‹
        
        # è®¡ç®—è¯¯å·®
        error = torch.norm(z_pred - z_1).item()
        z_1_norm = torch.norm(z_1).item()
        rel_error = error / (z_1_norm + 1e-8)
        
        errors.append(error)
        relative_errors.append(rel_error)
    
    if errors:
        logger.info(f"  é‡‡æ ·æ•°: {len(errors)}")
        logger.info(f"  ç»å¯¹è¯¯å·®: {np.mean(errors):.4f} Â± {np.std(errors):.4f}")
        logger.info(f"  ç›¸å¯¹è¯¯å·®: {np.mean(relative_errors):.2%} Â± {np.std(relative_errors):.2%}")
        
        # è¯„åˆ¤æ ‡å‡†
        if np.mean(relative_errors) < 0.1:
            logger.info("  âœ“ é‡å»ºè´¨é‡: ä¼˜ç§€ (ç›¸å¯¹è¯¯å·® < 10%)")
        elif np.mean(relative_errors) < 0.2:
            logger.info("  âœ“ é‡å»ºè´¨é‡: è‰¯å¥½ (ç›¸å¯¹è¯¯å·® < 20%)")
        else:
            logger.info("  âœ— é‡å»ºè´¨é‡: éœ€æ”¹è¿› (ç›¸å¯¹è¯¯å·® >= 20%)")
    
    return {'mean_error': np.mean(errors), 'mean_rel_error': np.mean(relative_errors)}


@torch.no_grad()
def validate_health_score_curve(model, latent_vectors, cycles, num_samples=20, device='cuda'):
    """
    éªŒè¯ 2: å¥åº·è¯„åˆ†æ›²çº¿
    
    æ£€æŸ¥é¢„æµ‹çš„å¥åº·æ›²çº¿æ˜¯å¦æ»¡è¶³:
    1. å•è°ƒé€’å¢ (é€€åŒ–åº”è¯¥è¶Šæ¥è¶Šä¸¥é‡)
    2. èŒƒå›´åœ¨ [0, 1] å†…
    3. æ—©æœŸ cycle è¯„åˆ†ä½ï¼Œæ™šæœŸ cycle è¯„åˆ†é«˜
    """
    logger.info("=" * 60)
    logger.info("éªŒè¯ 2: å¥åº·è¯„åˆ†æ›²çº¿ (å•è°ƒæ€§ & ç‰©ç†æ„ä¹‰)")
    logger.info("=" * 60)
    
    monotonic_count = 0
    valid_range_count = 0
    early_late_correct = 0
    
    # è·å–æ—©æœŸå’Œæ™šæœŸçš„ cycle
    min_cycle = cycles.min().item()
    max_cycle = cycles.max().item()
    early_threshold = min_cycle + (max_cycle - min_cycle) * 0.2
    late_threshold = min_cycle + (max_cycle - min_cycle) * 0.8
    
    for _ in range(num_samples):
        # éšæœºé€‰æ‹©ä¸€ä¸ªæ—©æœŸæ ·æœ¬
        early_mask = cycles <= early_threshold
        early_indices = torch.where(early_mask)[0]
        if len(early_indices) == 0:
            continue
        
        idx = early_indices[np.random.randint(len(early_indices))].item()
        z_0 = latent_vectors[idx:idx+1].to(device)
        
        # é¢„æµ‹è½¨è¿¹
        t_span = torch.linspace(0, 1, 100, device=device)
        trajectory = model.predict_trajectory(z_0, t_span)
        
        # è®¡ç®—å¥åº·è¯„åˆ†
        health_scores = []
        for z_t in trajectory:
            score = model.health_head(z_t).squeeze().item()
            health_scores.append(score)
        
        health_scores = np.array(health_scores)
        
        # æ£€æŸ¥å•è°ƒæ€§ (å…è®¸ä¸€äº›æ³¢åŠ¨)
        diffs = np.diff(health_scores)
        monotonic_ratio = np.mean(diffs >= -0.01)  # å…è®¸å¾®å°ä¸‹é™
        if monotonic_ratio > 0.9:
            monotonic_count += 1
        
        # æ£€æŸ¥èŒƒå›´
        if health_scores.min() >= -0.1 and health_scores.max() <= 1.1:
            valid_range_count += 1
        
        # æ£€æŸ¥æ—©æœŸä½ã€æ™šæœŸé«˜
        early_score = health_scores[:20].mean()
        late_score = health_scores[-20:].mean()
        if late_score > early_score:
            early_late_correct += 1
    
    logger.info(f"  é‡‡æ ·æ•°: {num_samples}")
    logger.info(f"  å•è°ƒé€’å¢æ¯”ä¾‹: {monotonic_count}/{num_samples} ({monotonic_count/num_samples:.1%})")
    logger.info(f"  èŒƒå›´æœ‰æ•ˆæ¯”ä¾‹: {valid_range_count}/{num_samples} ({valid_range_count/num_samples:.1%})")
    logger.info(f"  æ—©ä½æ™šé«˜æ¯”ä¾‹: {early_late_correct}/{num_samples} ({early_late_correct/num_samples:.1%})")
    
    if monotonic_count/num_samples > 0.8 and early_late_correct/num_samples > 0.9:
        logger.info("  âœ“ å¥åº·æ›²çº¿ç¬¦åˆç‰©ç†æ„ä¹‰")
    else:
        logger.info("  âœ— å¥åº·æ›²çº¿éœ€è¦æ£€æŸ¥")
    
    return {
        'monotonic_ratio': monotonic_count/num_samples,
        'valid_range_ratio': valid_range_count/num_samples,
        'early_late_ratio': early_late_correct/num_samples,
    }


@torch.no_grad()
def validate_latent_space_structure(model, latent_vectors, cycles, device='cuda'):
    """
    éªŒè¯ 3: æ½œç©ºé—´ç»“æ„
    
    æ£€æŸ¥:
    1. ä¸åŒ cycle çš„æ½œå‘é‡æ˜¯å¦æœ‰åºåˆ†å¸ƒ
    2. è½¨è¿¹æ˜¯å¦å¹³æ»‘è¿ç»­
    """
    logger.info("=" * 60)
    logger.info("éªŒè¯ 3: æ½œç©ºé—´ç»“æ„")
    logger.info("=" * 60)
    
    try:
        from sklearn.decomposition import PCA
        import matplotlib.pyplot as plt
    except ImportError:
        logger.warning("éœ€è¦ sklearn å’Œ matplotlib è¿›è¡Œå¯è§†åŒ–")
        return {}
    
    # PCA é™ç»´
    latent_np = latent_vectors.cpu().numpy()
    cycles_np = cycles.cpu().numpy()
    
    pca = PCA(n_components=2)
    latent_2d = pca.fit_transform(latent_np)
    
    # è®¡ç®—è§£é‡Šæ–¹å·®
    explained_var = pca.explained_variance_ratio_.sum()
    logger.info(f"  PCA 2D è§£é‡Šæ–¹å·®: {explained_var:.1%}")
    
    # è®¡ç®— cycle ä¸ PC1 çš„ç›¸å…³æ€§
    from scipy.stats import spearmanr
    corr, p_value = spearmanr(cycles_np, latent_2d[:, 0])
    logger.info(f"  Cycle vs PC1 ç›¸å…³æ€§: {corr:.3f} (p={p_value:.2e})")
    
    if abs(corr) > 0.5:
        logger.info("  âœ“ æ½œç©ºé—´ç»“æ„: cycle ä¸ PC1 æ˜¾è‘—ç›¸å…³ï¼Œè¯´æ˜é€€åŒ–æ–¹å‘å·²è¢«å­¦ä¹ ")
    else:
        logger.info("  âš  æ½œç©ºé—´ç»“æ„: cycle ä¸ PC1 ç›¸å…³æ€§è¾ƒå¼±")
    
    # å¯è§†åŒ–
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))
    
    # å·¦å›¾: æ‰€æœ‰æ ·æœ¬çš„æ½œç©ºé—´åˆ†å¸ƒ
    scatter = axes[0].scatter(latent_2d[:, 0], latent_2d[:, 1], c=cycles_np, cmap='viridis', alpha=0.6)
    axes[0].set_xlabel('PC1')
    axes[0].set_ylabel('PC2')
    axes[0].set_title('Latent Space Distribution (colored by Cycle)')
    plt.colorbar(scatter, ax=axes[0], label='Cycle')
    
    # å³å›¾: é¢„æµ‹è½¨è¿¹
    # é€‰æ‹©ä¸€ä¸ªæ—©æœŸæ ·æœ¬ï¼Œé¢„æµ‹å…¶è½¨è¿¹
    early_idx = cycles_np.argmin()
    z_0 = latent_vectors[early_idx:early_idx+1].to(device)
    t_span = torch.linspace(0, 1, 100, device=device)
    trajectory = model.predict_trajectory(z_0, t_span).cpu().numpy()[:, 0, :]
    trajectory_2d = pca.transform(trajectory)
    
    axes[1].scatter(latent_2d[:, 0], latent_2d[:, 1], c=cycles_np, cmap='viridis', alpha=0.3)
    axes[1].plot(trajectory_2d[:, 0], trajectory_2d[:, 1], 'r-', linewidth=2, label='Predicted Trajectory')
    axes[1].scatter(trajectory_2d[0, 0], trajectory_2d[0, 1], c='green', s=100, marker='o', label='Start (t=0)')
    axes[1].scatter(trajectory_2d[-1, 0], trajectory_2d[-1, 1], c='red', s=100, marker='x', label='End (t=1)')
    axes[1].set_xlabel('PC1')
    axes[1].set_ylabel('PC2')
    axes[1].set_title('Predicted Trajectory in Latent Space')
    axes[1].legend()
    
    plt.tight_layout()
    
    # ä¿å­˜
    save_path = Path('experiments/flow_matching/validation')
    save_path.mkdir(parents=True, exist_ok=True)
    plt.savefig(save_path / 'latent_space_validation.png', dpi=150, bbox_inches='tight')
    logger.info(f"  å¯è§†åŒ–å·²ä¿å­˜: {save_path / 'latent_space_validation.png'}")
    plt.close()
    
    return {'explained_variance': explained_var, 'cycle_pc1_correlation': corr}


@torch.no_grad()
def validate_rul_prediction(model, latent_vectors, cycles, num_samples=50, device='cuda'):
    """
    éªŒè¯ 4: RUL é¢„æµ‹ç²¾åº¦
    
    RUL (Remaining Useful Life) é¢„æµ‹æ–¹æ³•:
    1. ç»™å®šå½“å‰ cycle çš„æ½œå‘é‡ z_current
    2. ä½¿ç”¨ Flow Model é¢„æµ‹æœªæ¥è½¨è¿¹
    3. é€šè¿‡æ¯”è¾ƒé¢„æµ‹è½¨è¿¹ä¸å·²çŸ¥æ™šæœŸæ ·æœ¬çš„ç›¸ä¼¼åº¦ï¼Œä¼°è®¡åˆ°è¾¾é€€åŒ–çŠ¶æ€çš„ cycle
    4. ä¸çœŸå®çš„å‰©ä½™ cycle æ¯”è¾ƒ
    """
    logger.info("=" * 60)
    logger.info("éªŒè¯ 4: RUL (å‰©ä½™å¯¿å‘½) é¢„æµ‹èƒ½åŠ›")
    logger.info("=" * 60)
    
    max_cycle = cycles.max().item()
    min_cycle = cycles.min().item()
    total_cycles = max_cycle - min_cycle
    
    # è·å–æ™šæœŸæ ·æœ¬ä½œä¸ºå‚è€ƒ (æœ€å 10% çš„ cycle)
    late_threshold = min_cycle + total_cycles * 0.9
    late_mask = cycles >= late_threshold
    late_indices = torch.where(late_mask)[0]
    
    if len(late_indices) < 5:
        logger.warning("  æ²¡æœ‰è¶³å¤Ÿçš„æ™šæœŸæ ·æœ¬ä½œä¸ºå‚è€ƒ")
        return {}
    
    # è®¡ç®—æ™šæœŸæ ·æœ¬çš„å¹³å‡æ½œå‘é‡ä½œä¸º "å¤±æ•ˆçŠ¶æ€" å‚è€ƒ
    late_vectors = latent_vectors[late_indices].to(device)
    late_center = late_vectors.mean(dim=0, keepdim=True)
    
    # é€‰æ‹©ä¸­é—´é˜¶æ®µçš„æ ·æœ¬ä½œä¸ºæµ‹è¯• (30% - 70%)
    mid_start = min_cycle + total_cycles * 0.3
    mid_end = min_cycle + total_cycles * 0.7
    
    test_mask = (cycles >= mid_start) & (cycles <= mid_end)
    test_indices = torch.where(test_mask)[0]
    
    if len(test_indices) == 0:
        logger.warning("  æ²¡æœ‰è¶³å¤Ÿçš„ä¸­æœŸæ ·æœ¬è¿›è¡Œ RUL éªŒè¯")
        return {}
    
    # éšæœºé‡‡æ ·
    sample_indices = test_indices[torch.randperm(len(test_indices))[:min(num_samples, len(test_indices))]]
    
    rul_errors = []
    rul_relative_errors = []
    
    for idx in sample_indices:
        current_cycle = cycles[idx].item()
        z_0 = latent_vectors[idx:idx+1].to(device)
        
        # çœŸå® RUL (åˆ°æœ€å¤§è§‚æµ‹ cycle)
        true_rul = max_cycle - current_cycle
        
        if true_rul <= 0:
            continue
        
        # ä½¿ç”¨ Flow Model é¢„æµ‹æœªæ¥è½¨è¿¹
        # ä»å½“å‰å½’ä¸€åŒ–æ—¶é—´ t_current é¢„æµ‹åˆ° t=1
        t_current = (current_cycle - min_cycle) / total_cycles
        num_steps = 100
        t_span = torch.linspace(t_current, 1.0, num_steps, device=device)
        
        with torch.no_grad():
            trajectory = model.predict_trajectory(z_0, t_span)
        
        # è®¡ç®—è½¨è¿¹ä¸Šæ¯ä¸ªç‚¹åˆ°æ™šæœŸä¸­å¿ƒçš„è·ç¦»
        distances = []
        for z_t in trajectory:
            dist = torch.norm(z_t - late_center, dim=-1).item()
            distances.append(dist)
        
        distances = np.array(distances)
        initial_dist = distances[0]
        
        # æ‰¾åˆ°è·ç¦»å¼€å§‹æ˜æ˜¾å‡å°çš„ç‚¹ (æ¥è¿‘æ™šæœŸçŠ¶æ€)
        # ä½¿ç”¨ç›¸å¯¹äºåˆå§‹è·ç¦»çš„æ¯”ä¾‹
        if initial_dist > 0:
            relative_distances = distances / initial_dist
        else:
            relative_distances = distances
        
        # é¢„æµ‹å¤±æ•ˆç‚¹: å½“ç›¸å¯¹è·ç¦»ä¸‹é™åˆ°æŸä¸ªé˜ˆå€¼ (å¦‚ 0.3ï¼Œå³æ¥è¿‘æ™šæœŸçŠ¶æ€)
        failure_threshold = 0.5  # å½“è·ç¦»å‡å°‘åˆ° 50% æ—¶è®¤ä¸ºæ¥è¿‘å¤±æ•ˆ
        
        # æ‰¾åˆ°ç¬¬ä¸€ä¸ªæ»¡è¶³æ¡ä»¶çš„ç‚¹
        predicted_failure_idx = None
        for i, rel_dist in enumerate(relative_distances):
            if rel_dist < failure_threshold:
                predicted_failure_idx = i
                break
        
        if predicted_failure_idx is not None:
            # è®¡ç®—é¢„æµ‹çš„å¤±æ•ˆæ—¶é—´
            predicted_failure_t = t_span[predicted_failure_idx].item()
            predicted_failure_cycle = min_cycle + predicted_failure_t * total_cycles
            predicted_rul = predicted_failure_cycle - current_cycle
        else:
            # å¦‚æœæ²¡æœ‰åˆ°è¾¾å¤±æ•ˆé˜ˆå€¼ï¼Œä½¿ç”¨è½¨è¿¹ç»ˆç‚¹ä½œä¸ºä¼°è®¡
            # æ ¹æ®è·ç¦»è¡°å‡è¶‹åŠ¿å¤–æ¨
            if len(distances) >= 2 and distances[-1] < distances[0]:
                # è®¡ç®—è¡°å‡é€Ÿç‡
                decay_rate = (distances[0] - distances[-1]) / (len(distances) - 1)
                if decay_rate > 0:
                    # ä¼°è®¡è¿˜éœ€è¦å¤šå°‘æ­¥åˆ°è¾¾å¤±æ•ˆ
                    remaining_dist = distances[-1] - (initial_dist * failure_threshold)
                    if remaining_dist > 0:
                        extra_steps = remaining_dist / decay_rate
                        extra_cycles = extra_steps / num_steps * (1.0 - t_current) * total_cycles
                        predicted_rul = true_rul + extra_cycles
                    else:
                        predicted_rul = true_rul * 0.9  # å¿«åˆ°äº†
                else:
                    predicted_rul = true_rul * 1.5  # è¡°å‡å¾ˆæ…¢ï¼Œå¯èƒ½è¿˜æœ‰æ›´é•¿å¯¿å‘½
            else:
                # æ²¡æœ‰æ˜æ˜¾è¡°å‡è¶‹åŠ¿ï¼Œå¯èƒ½æ¨¡å‹æ²¡å­¦å¥½ï¼Œä½¿ç”¨å¹³å‡ä¼°è®¡
                predicted_rul = true_rul * 1.2
        
        # ç¡®ä¿é¢„æµ‹å€¼åˆç†
        predicted_rul = max(0, predicted_rul)
        
        rul_error = abs(predicted_rul - true_rul)
        rul_relative_error = rul_error / true_rul if true_rul > 0 else 0
        
        rul_errors.append(rul_error)
        rul_relative_errors.append(rul_relative_error)
    
    if rul_errors:
        mean_error = np.mean(rul_errors)
        std_error = np.std(rul_errors)
        mean_relative_error = np.mean(rul_relative_errors)
        
        logger.info(f"  æµ‹è¯•æ ·æœ¬æ•°: {len(rul_errors)}")
        logger.info(f"  RUL è¯¯å·® (cycles): {mean_error:.1f} Â± {std_error:.1f}")
        logger.info(f"  RUL ç›¸å¯¹è¯¯å·®: {mean_relative_error:.1%}")
        logger.info(f"  æœ€å¤§è¯¯å·®: {max(rul_errors):.1f} cycles")
        logger.info(f"  æœ€å°è¯¯å·®: {min(rul_errors):.1f} cycles")
        
        if mean_relative_error < 0.15:
            logger.info("  âœ“ RUL é¢„æµ‹: ç²¾åº¦è¾ƒå¥½ (< 15% ç›¸å¯¹è¯¯å·®)")
        elif mean_relative_error < 0.25:
            logger.info("  â–³ RUL é¢„æµ‹: ç²¾åº¦ä¸€èˆ¬ (15%-25% ç›¸å¯¹è¯¯å·®)")
        else:
            logger.info("  âš  RUL é¢„æµ‹: è¯¯å·®è¾ƒå¤§ï¼Œå¯èƒ½éœ€è¦æ›´å¤šè®­ç»ƒ")
        
        return {
            'mean_rul_error': mean_error,
            'std_rul_error': std_error,
            'mean_relative_error': mean_relative_error,
            'num_samples': len(rul_errors)
        }
    
    return {}


def generate_demo_prediction(model, latent_vectors, cycles, device='cuda'):
    """
    ç”Ÿæˆæ¼”ç¤ºé¢„æµ‹å›¾
    
    å±•ç¤ºä»ä¸åŒåˆå§‹ cycle å‡ºå‘çš„é¢„æµ‹è½¨è¿¹
    ä½¿ç”¨æ½œç©ºé—´è·ç¦»å˜åŒ–æ¥å¯è§†åŒ–é€€åŒ–è¶‹åŠ¿ï¼ˆè€Œä¸æ˜¯æœªè®­ç»ƒçš„ health_headï¼‰
    """
    logger.info("=" * 60)
    logger.info("ç”Ÿæˆæ¼”ç¤ºé¢„æµ‹å›¾")
    logger.info("=" * 60)
    
    try:
        import matplotlib.pyplot as plt
    except ImportError:
        logger.warning("éœ€è¦ matplotlib")
        return
    
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))
    
    max_cycle = cycles.max().item()
    min_cycle = cycles.min().item()
    total_cycles = max_cycle - min_cycle
    
    # è®¡ç®—æ—©æœŸå’Œæ™šæœŸå‚è€ƒç‚¹
    early_threshold = min_cycle + total_cycles * 0.1
    late_threshold = min_cycle + total_cycles * 0.9
    
    early_mask = cycles <= early_threshold
    late_mask = cycles >= late_threshold
    
    early_indices = torch.where(early_mask)[0]
    late_indices = torch.where(late_mask)[0]
    
    if len(early_indices) == 0 or len(late_indices) == 0:
        logger.warning("  æ²¡æœ‰è¶³å¤Ÿçš„æ—©æœŸ/æ™šæœŸæ ·æœ¬")
        return
    
    # è®¡ç®—å‚è€ƒç‚¹
    early_center = latent_vectors[early_indices].mean(dim=0, keepdim=True).to(device)
    late_center = latent_vectors[late_indices].mean(dim=0, keepdim=True).to(device)
    
    # é€‰æ‹©ä¸åŒé˜¶æ®µçš„èµ·ç‚¹
    percentiles = [0.1, 0.3, 0.5, 0.7]
    colors = ['blue', 'green', 'orange', 'red']
    
    # è®¡ç®—æ—©æœŸåˆ°æ™šæœŸçš„å‚è€ƒè·ç¦»
    ref_dist = torch.norm(early_center - late_center).item()
    
    for ax_idx, (pct, color) in enumerate(zip(percentiles, colors)):
        ax = axes[ax_idx // 2, ax_idx % 2]
        
        target_cycle = min_cycle + total_cycles * pct
        idx = (torch.abs(cycles - target_cycle)).argmin().item()
        start_cycle = cycles[idx].item()
        
        z_0 = latent_vectors[idx:idx+1].to(device)
        
        # é¢„æµ‹è½¨è¿¹
        t_start = (start_cycle - min_cycle) / total_cycles
        t_span = torch.linspace(t_start, 1.0, 100, device=device)
        
        with torch.no_grad():
            trajectory = model.predict_trajectory(z_0, t_span)
        
        # è®¡ç®—é€€åŒ–æŒ‡æ ‡ï¼šåŸºäºåˆ°æ™šæœŸä¸­å¿ƒçš„è·ç¦»å˜åŒ–
        # é€€åŒ–ç¨‹åº¦ = (åˆå§‹åˆ°æ™šæœŸè·ç¦» - å½“å‰åˆ°æ™šæœŸè·ç¦») / å‚è€ƒè·ç¦»
        degradation_scores = []
        initial_dist_to_late = torch.norm(z_0 - late_center).item()
        
        for z_t in trajectory:
            dist_to_late = torch.norm(z_t - late_center).item()
            dist_to_early = torch.norm(z_t - early_center).item()
            
            # è®¡ç®—ç›¸å¯¹ä½ç½®ï¼š0 = æ¥è¿‘æ—©æœŸ, 1 = æ¥è¿‘æ™šæœŸ
            # ä½¿ç”¨è·ç¦»æ¯”ä¾‹
            total_dist = dist_to_early + dist_to_late + 1e-6
            degradation = dist_to_early / total_dist  # è¶Šæ¥è¿‘æ™šæœŸï¼Œdist_to_early è¶Šå¤§
            
            degradation_scores.append(degradation)
        
        predicted_cycles = t_span.cpu().numpy() * total_cycles + min_cycle
        
        ax.plot(predicted_cycles, degradation_scores, color=color, linewidth=2, 
                label=f'From Cycle {int(start_cycle)}')
        ax.axhline(y=0.8, color='gray', linestyle='--', alpha=0.5, label='Failure Threshold')
        ax.axvline(x=start_cycle, color=color, linestyle=':', alpha=0.5)
        
        ax.set_xlabel('Cycle')
        ax.set_ylabel('Degradation Level')
        ax.set_title(f'Starting from Cycle {int(start_cycle)} ({pct:.0%} of lifecycle)')
        ax.set_ylim(-0.05, 1.05)
        ax.legend()
        ax.grid(True, alpha=0.3)
    
    plt.suptitle('Flow Matching Lifecycle Predictions (Latent Space Distance)', fontsize=14)
    plt.tight_layout()
    
    save_path = Path('experiments/flow_matching/validation')
    save_path.mkdir(parents=True, exist_ok=True)
    plt.savefig(save_path / 'lifecycle_predictions.png', dpi=150, bbox_inches='tight')
    logger.info(f"  é¢„æµ‹å›¾å·²ä¿å­˜: {save_path / 'lifecycle_predictions.png'}")
    plt.close()
    
    # é¢å¤–ç”Ÿæˆä¸€ä¸ªè½¨è¿¹å¯¹æ¯”å›¾
    _generate_trajectory_comparison(model, latent_vectors, cycles, device, save_path)


def _generate_trajectory_comparison(model, latent_vectors, cycles, device, save_path):
    """ç”Ÿæˆè½¨è¿¹å¯¹æ¯”å›¾ï¼šçœŸå® vs é¢„æµ‹"""
    try:
        import matplotlib.pyplot as plt
        from sklearn.decomposition import PCA
    except ImportError:
        return
    
    # PCA é™ç»´
    pca = PCA(n_components=2)
    latent_2d = pca.fit_transform(latent_vectors.cpu().numpy())
    
    fig, axes = plt.subplots(1, 2, figsize=(14, 6))
    
    max_cycle = cycles.max().item()
    min_cycle = cycles.min().item()
    total_cycles = max_cycle - min_cycle
    
    # å·¦å›¾ï¼šçœŸå®æ½œç©ºé—´åˆ†å¸ƒ
    ax1 = axes[0]
    scatter = ax1.scatter(latent_2d[:, 0], latent_2d[:, 1], c=cycles.numpy(), 
                          cmap='viridis', alpha=0.6, s=20)
    plt.colorbar(scatter, ax=ax1, label='Cycle')
    ax1.set_xlabel('PC1')
    ax1.set_ylabel('PC2')
    ax1.set_title('True Latent Space Distribution')
    ax1.grid(True, alpha=0.3)
    
    # å³å›¾ï¼šé¢„æµ‹è½¨è¿¹
    ax2 = axes[1]
    ax2.scatter(latent_2d[:, 0], latent_2d[:, 1], c='lightgray', alpha=0.3, s=10, label='True samples')
    
    # ä»å‡ ä¸ªèµ·ç‚¹é¢„æµ‹è½¨è¿¹
    colors = ['red', 'blue', 'green', 'purple']
    start_pcts = [0.1, 0.3, 0.5, 0.7]
    
    for start_pct, color in zip(start_pcts, colors):
        target_cycle = min_cycle + total_cycles * start_pct
        idx = (torch.abs(cycles - target_cycle)).argmin().item()
        start_cycle = cycles[idx].item()
        
        z_0 = latent_vectors[idx:idx+1].to(device)
        
        t_start = (start_cycle - min_cycle) / total_cycles
        t_span = torch.linspace(t_start, 1.0, 50, device=device)
        
        with torch.no_grad():
            trajectory = model.predict_trajectory(z_0, t_span)
        
        # è½¬æ¢åˆ° 2D
        if isinstance(trajectory, torch.Tensor):
            traj_np = trajectory.squeeze(1).cpu().numpy()
        else:
            traj_np = torch.stack(trajectory).squeeze(1).cpu().numpy()
        traj_2d = pca.transform(traj_np)
        
        ax2.plot(traj_2d[:, 0], traj_2d[:, 1], color=color, linewidth=2, 
                 label=f'Pred from Cycle {int(start_cycle)}')
        ax2.scatter(traj_2d[0, 0], traj_2d[0, 1], color=color, s=100, marker='o', edgecolors='black')
        ax2.scatter(traj_2d[-1, 0], traj_2d[-1, 1], color=color, s=100, marker='*', edgecolors='black')
    
    ax2.set_xlabel('PC1')
    ax2.set_ylabel('PC2')
    ax2.set_title('Predicted Trajectories in Latent Space')
    ax2.legend(loc='upper right')
    ax2.grid(True, alpha=0.3)
    
    plt.suptitle('Latent Space: True Distribution vs Predicted Trajectories', fontsize=14)
    plt.tight_layout()
    plt.savefig(save_path / 'trajectory_comparison.png', dpi=150, bbox_inches='tight')
    logger.info(f"  è½¨è¿¹å¯¹æ¯”å›¾å·²ä¿å­˜: {save_path / 'trajectory_comparison.png'}")
    plt.close()


def main():
    parser = argparse.ArgumentParser(description="Flow Matching Model Validation")
    parser.add_argument('--checkpoint', type=str, required=True, help='Flow Matching æ¨¡å‹æ£€æŸ¥ç‚¹')
    parser.add_argument('--encoder_checkpoint', type=str, default='latest.pth', help='Encoder æ£€æŸ¥ç‚¹')
    parser.add_argument('--cache_dir', type=str, default='experiments/flow_matching/cache', 
                        help='æ½œç©ºé—´ç¼“å­˜ç›®å½•')
    parser.add_argument('--device', type=str, default='cuda', help='è®¾å¤‡')
    args = parser.parse_args()
    
    device = torch.device(args.device if torch.cuda.is_available() else 'cpu')
    logger.info(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # 1. åŠ è½½ Encoder
    logger.info("åŠ è½½ Encoder...")
    encoder, latent_dim = load_encoder(args.encoder_checkpoint, device)
    
    # 2. åŠ è½½ Flow Matching æ¨¡å‹
    logger.info("åŠ è½½ Flow Matching æ¨¡å‹...")
    model, config = load_flow_model(args.checkpoint, encoder, latent_dim, device)
    
    # 3. åŠ è½½æ½œç©ºé—´ç¼“å­˜
    logger.info("åŠ è½½æ½œç©ºé—´ç¼“å­˜...")
    cache_dir = Path(args.cache_dir)
    if not (cache_dir / 'latent_vectors.pt').exists():
        logger.error(f"ç¼“å­˜ä¸å­˜åœ¨: {cache_dir}")
        logger.info("è¯·å…ˆè¿è¡Œè®­ç»ƒè„šæœ¬ç”Ÿæˆç¼“å­˜: python scripts/train_flow.py --compute_cache")
        return
    
    cache_data = torch.load(cache_dir / 'latent_vectors.pt')
    latent_vectors = cache_data['latent_vectors']
    cycles = cache_data['cycles']
    
    logger.info(f"åŠ è½½ {len(latent_vectors)} ä¸ªæ½œå‘é‡")
    logger.info(f"Cycle èŒƒå›´: {cycles.min().item()} - {cycles.max().item()}")
    
    # 4. è¿è¡ŒéªŒè¯
    logger.info("\n" + "=" * 60)
    logger.info("å¼€å§‹ Flow Matching æ¨¡å‹éªŒè¯")
    logger.info("=" * 60 + "\n")
    
    results = {}
    
    # éªŒè¯ 1: è½¨è¿¹é‡å»º
    results['reconstruction'] = validate_trajectory_reconstruction(
        model, latent_vectors, cycles, num_samples=100, device=device
    )
    
    # éªŒè¯ 2: å¥åº·æ›²çº¿
    results['health_curve'] = validate_health_score_curve(
        model, latent_vectors, cycles, num_samples=20, device=device
    )
    
    # éªŒè¯ 3: æ½œç©ºé—´ç»“æ„
    results['latent_structure'] = validate_latent_space_structure(
        model, latent_vectors, cycles, device=device
    )
    
    # éªŒè¯ 4: RUL é¢„æµ‹
    results['rul'] = validate_rul_prediction(
        model, latent_vectors, cycles, num_samples=50, device=device
    )
    
    # ç”Ÿæˆæ¼”ç¤ºå›¾
    generate_demo_prediction(model, latent_vectors, cycles, device=device)
    
    # æ€»ç»“
    logger.info("\n" + "=" * 60)
    logger.info("éªŒè¯æ€»ç»“")
    logger.info("=" * 60)
    
    all_passed = True
    
    if results['reconstruction'].get('mean_rel_error', 1) < 0.2:
        logger.info("âœ“ è½¨è¿¹é‡å»º: é€šè¿‡")
    else:
        logger.info("âœ— è½¨è¿¹é‡å»º: éœ€æ”¹è¿›")
        all_passed = False
    
    if results['health_curve'].get('monotonic_ratio', 0) > 0.8:
        logger.info("âœ“ å¥åº·æ›²çº¿: é€šè¿‡")
    else:
        logger.info("âœ— å¥åº·æ›²çº¿: éœ€æ”¹è¿›")
        all_passed = False
    
    if abs(results['latent_structure'].get('cycle_pc1_correlation', 0)) > 0.5:
        logger.info("âœ“ æ½œç©ºé—´ç»“æ„: é€šè¿‡")
    else:
        logger.info("âš  æ½œç©ºé—´ç»“æ„: ç›¸å…³æ€§è¾ƒå¼±")
    
    if all_passed:
        logger.info("\nğŸ‰ æ¨¡å‹éªŒè¯é€šè¿‡ï¼å¯ä»¥è¿›è¡Œæ¨ç†éƒ¨ç½²ã€‚")
    else:
        logger.info("\nâš  éƒ¨åˆ†éªŒè¯æœªé€šè¿‡ï¼Œå»ºè®®ç»§ç»­è®­ç»ƒæˆ–è°ƒæ•´è¶…å‚æ•°ã€‚")


if __name__ == "__main__":
    main()
